phoneme
# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 13098195038893955749), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 11877778986693481218), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6050445799678666652)]
# Loading hparams from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# Vocab file /home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.in exists
# Vocab file /home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.out exists
The first 3 vocab words [(, ), Move] are not [<unk>, <s>, </s>]
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/best_bleu/hparams
  attention=
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=55.70508073759653
  best_bleu_dir=/home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  decay_scheme=
  dev_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/devdata
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=80
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=1
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=1
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=12000
  num_translations_per_input=1
  num_units=128
  optimizer=sgd
  out_dir=/home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=in
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.in
  src_vocab_size=87
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/testdata
  tgt=out
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/Vocab.out
  tgt_vocab_size=55
  time_major=True
  train_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/traindata
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/Vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/decoder/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), 
# log_file=/home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/log_1547693285
  loaded train model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.07s
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.03s
  # 439
    src: M UW1 V T UW1 L IH1 V IH0 NG T EY1 B AH0 L G R AE1 S P G R IY1 N T IY1 L IY1 V DH AH0 AH0 P AA1 R T M AH0 N T P L IY1 Z
    ref: Move ( Living_Table )  Grasp ( Green_Tea )  Move ( apartment )
    nmt: Move ( Living_Table ) Grasp ( Green_Tea ) Move ( apartment )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.03s
  eval dev: perplexity 1.00, time 0s, Thu Jan 17 11:48:07 2019.
  eval test: perplexity 1.22, time 0s, Thu Jan 17 11:48:07 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.01s
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:09 2019.
  bleu dev: 55.7
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:10 2019.
  bleu test: 49.3
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# Start step 12000, lr 1, Thu Jan 17 11:48:10 2019
# Init train iterator, skipping 10240 elements
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.02s
  # 187
    src: G OW1 T UW1 R IH0 S EH1 P SH AH0 N T EY1 B AH0 L M UW1 V T UW1 S OW1 F AH0 T EY1 K B AE1 TH K L IY1 N ER0
    ref: Move ( Reception_Table )  Move ( Sofa )  Find ( Bath_Cleaner )  Grasp ( Bath_Cleaner )  Comeback ( )
    nmt: Move ( Reception_Table ) Move ( Sofa ) Find ( Bath_Cleaner ) Grasp ( Bath_Cleaner ) Comeback ( )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.02s
  eval dev: perplexity 1.00, time 0s, Thu Jan 17 11:48:11 2019.
  eval test: perplexity 1.22, time 0s, Thu Jan 17 11:48:12 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/translate.ckpt-12000, time 0.01s
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:13 2019.
  bleu dev: 55.7
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:15 2019.
  bleu test: 49.3
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# Final, step 12000 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 1.00, dev bleu 55.7, test ppl 1.22, test bleu 49.3, Thu Jan 17 11:48:15 2019
# Done training!, time 4s, Thu Jan 17 11:48:15 2019.
# Start evaluating saved best models.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/best_bleu/translate.ckpt-6000, time 0.02s
  # 1930
    src: M UW1 V T UW1 L IH1 V IH0 NG T EY1 B AH0 L G EH1 T S T R AO1 B EH2 R IY0 JH UW1 S EH1 G Z IH0 T F R AH1 M V IH1 Z IH0 T ER0 R UW1 M OW2 K EY1
    ref: Move ( Living_Table )  grasp ( Strawberry_Juice )  Move ( Visitor_Room )
    nmt: Move ( Living_Table ) grasp ( Strawberry_Juice ) Move ( Visitor_Room )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/best_bleu/translate.ckpt-6000, time 0.02s
  eval dev: perplexity 1.01, time 0s, Thu Jan 17 11:48:16 2019.
  eval test: perplexity 1.20, time 0s, Thu Jan 17 11:48:17 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/best_bleu/translate.ckpt-6000, time 0.02s
# External evaluation, global step 6000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:18 2019.
  bleu dev: 55.7
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# External evaluation, global step 6000
  decoding to output /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 1s, Thu Jan 17 11:48:19 2019.
  bleu test: 48.9
  saving hparams to /home/emlab/Desktop/syuuron/nmt/Noattention_phoneme_model/hparams
# Best bleu, step 6000 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 1.01, dev bleu 55.7, test ppl 1.20, test bleu 48.9, Thu Jan 17 11:48:20 2019
# Job id 0
# Devices visible to TensorFlow: [_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 4162532740831001650), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 14896911282125087031), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 13959413406203373851)]
# Loading hparams from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# Vocab file /home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.in exists
# Vocab file /home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.out exists
The first 3 vocab words [(, ), Move] are not [<unk>, <s>, </s>]
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/best_bleu/hparams
  attention=luong
  attention_architecture=standard
  avg_ckpts=False
  batch_size=128
  beam_width=0
  best_bleu=55.91927764798287
  best_bleu_dir=/home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/best_bleu
  check_special_token=True
  colocate_gradients_with_ops=True
  decay_scheme=
  dev_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/devdata
  dropout=0.2
  embed_prefix=None
  encoder_type=uni
  eos=</s>
  epoch_step=80
  forget_bias=1.0
  infer_batch_size=32
  infer_mode=greedy
  init_op=uniform
  init_weight=0.1
  language_model=False
  learning_rate=1.0
  length_penalty_weight=0.0
  log_device_placement=False
  max_gradient_norm=5.0
  max_train=0
  metrics=['bleu']
  num_buckets=5
  num_dec_emb_partitions=0
  num_decoder_layers=1
  num_decoder_residual_layers=0
  num_embeddings_partitions=0
  num_enc_emb_partitions=0
  num_encoder_layers=1
  num_encoder_residual_layers=0
  num_gpus=1
  num_inter_threads=0
  num_intra_threads=0
  num_keep_ckpts=5
  num_sampled_softmax=0
  num_train_steps=12000
  num_translations_per_input=1
  num_units=128
  optimizer=sgd
  out_dir=/home/emlab/Desktop/syuuron/nmt/attention_phoneme_model
  output_attention=True
  override_loaded_hparams=False
  pass_hidden_state=True
  random_seed=None
  residual=False
  sampling_temperature=0.0
  share_vocab=False
  sos=<s>
  src=in
  src_embed_file=
  src_max_len=50
  src_max_len_infer=None
  src_vocab_file=/home/emlab/Desktop/syuuron/nmt/Dataset/Vocab.in
  src_vocab_size=87
  steps_per_external_eval=None
  steps_per_stats=100
  subword_option=
  test_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/testdata
  tgt=out
  tgt_embed_file=
  tgt_max_len=50
  tgt_max_len_infer=None
  tgt_vocab_file=/home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/Vocab.out
  tgt_vocab_size=55
  time_major=True
  train_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/traindata
  unit_type=lstm
  use_char_encode=False
  vocab_prefix=/home/emlab/Desktop/syuuron/nmt/Dataset/Vocab
  warmup_scheme=t2t
  warmup_steps=0
# Creating train graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0
  learning_rate=1, warmup_steps=0, warmup_scheme=t2t
  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), /device:GPU:0
# Creating eval graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), /device:GPU:0
# Creating infer graph ...
# Build a basic encoder
  num_layers = 1, num_residual_layers=0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0
  decoder: infer_mode=greedybeam_width=0, length_penalty=0.000000
# Trainable variables
Format: <name>, <shape>, <(soft) device placement>
  embeddings/encoder/embedding_encoder:0, (87, 128), /device:GPU:0
  embeddings/decoder/embedding_decoder:0, (55, 128), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0
  dynamic_seq2seq/encoder/rnn/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), 
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0
  dynamic_seq2seq/decoder/attention/basic_lstm_cell/bias:0, (512,), /device:GPU:0
  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0
  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 55), 
# log_file=/home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/log_1547693305
  loaded train model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.11s
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.05s
  # 182
    src: G OW1 T UW1 L IH1 V IH0 NG S OW1 F AH0 N EH1 K S T D IH0 T EH1 K T K L AO1 TH K L IY1 N ER0 T EY1 K IH1 T
    ref: Move ( Living_Sofa )  Find ( Cloth_Cleaner )  Grasp ( Cloth_Cleaner )
    nmt: Move ( Living_Sofa ) Find ( Cloth_Cleaner ) Grasp ( Cloth_Cleaner )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.05s
  eval dev: perplexity 1.00, time 1s, Thu Jan 17 11:48:27 2019.
  eval test: perplexity 1.16, time 1s, Thu Jan 17 11:48:28 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.03s
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:31 2019.
  bleu dev: 55.9
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:34 2019.
  bleu test: 51.4
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# Start step 12000, lr 1, Thu Jan 17 11:48:34 2019
# Init train iterator, skipping 10240 elements
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.03s
  # 1108
    src: W UH1 D Y UW1 G OW1 T UW1 B AA1 R T EY1 K D IH1 SH K L IY1 N ER0 ER0 K AH1 M B AE1 K
    ref: Move ( Bar )  Find ( Dish_Cleaner )  Grasp ( Dish_Cleaner )  Comeback ( )
    nmt: Move ( Bar ) Find ( Dish_Cleaner ) Grasp ( Dish_Cleaner ) Comeback ( )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.04s
  eval dev: perplexity 1.00, time 1s, Thu Jan 17 11:48:36 2019.
  eval test: perplexity 1.16, time 1s, Thu Jan 17 11:48:38 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/translate.ckpt-12000, time 0.03s
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:40 2019.
  bleu dev: 55.9
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# External evaluation, global step 12000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:43 2019.
  bleu test: 51.4
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# Final, step 12000 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 1.00, dev bleu 55.9, test ppl 1.16, test bleu 51.4, Thu Jan 17 11:48:43 2019
# Done training!, time 9s, Thu Jan 17 11:48:43 2019.
# Start evaluating saved best models.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/best_bleu/translate.ckpt-2000, time 0.03s
  # 341
    src: M UW1 V T UW1 S AY1 D T EY1 B AH0 L N EH1 K S T D IH0 T EH1 K T AH1 N Y AH0 N D R EH1 S IH0 NG G EH1 T IH1 T
    ref: Move ( Side_Table )  Find ( Onion_Dressing )  Grasp ( Onion_Dressing )
    nmt: Move ( Side_Table ) Find ( Onion_Dressing ) Grasp ( Onion_Dressing )
  loaded eval model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/best_bleu/translate.ckpt-2000, time 0.03s
  eval dev: perplexity 1.00, time 1s, Thu Jan 17 11:48:45 2019.
  eval test: perplexity 1.12, time 1s, Thu Jan 17 11:48:46 2019.
  loaded infer model parameters from /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/best_bleu/translate.ckpt-2000, time 0.03s
# External evaluation, global step 2000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_dev
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:48 2019.
  bleu dev: 55.9
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# External evaluation, global step 2000
  decoding to output /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/output_test
  done, num sentences 2001, num translations per input 1, time 2s, Thu Jan 17 11:48:51 2019.
  bleu test: 52.0
  saving hparams to /home/emlab/Desktop/syuuron/nmt/attention_phoneme_model/hparams
# Best bleu, step 2000 lr 1 step-time 0.00s wps 0.00K ppl 0.00 gN 0.00 dev ppl 1.00, dev bleu 55.9, test ppl 1.12, test bleu 52.0, Thu Jan 17 11:48:51 2019
